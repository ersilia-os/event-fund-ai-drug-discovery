{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ersilia-os/event-fund-ai-drug-discovery/blob/main/notebooks/session2_skills.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiTOUrB6KLim"
      },
      "source": [
        "# Session 2: \n",
        "In this session, we will use a simple dataset to train a basic Machine Learning Model and understand the steps involved in their development.\n",
        "\n",
        "*Disclaimer: this pipeline is an example prepared with curated data and should not be reproduced with the student's own datasets. The goal of this workshop is purely academic and does not represent a real research case study.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFm_xu6NgFOQ"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "In this exercise, we will leverage the [Therapeutics Data Commons](https://tdcommons.ai/) database, an open science effort to collect AI-ready datasets for biomedical questions (read more on its [publication](https://arxiv.org/abs/2102.09548)).\n",
        "\n",
        "The data cleaning steps discussed in Session 1 have already been done for the TDC data, so we can focus directly on the generation of ML models. We will retrieve two toxicity-related datasets from TDC to train a classification model (2.1) and a regression model (2.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLV1joI8MK51"
      },
      "source": [
        "## 2.1 Classification Task\n",
        "A classification model identifies the \"category\" of a new datapoint. In most QSAR models, the model learns how to classify molecules in:\n",
        "\n",
        "\n",
        "*   Active: labelled with 1 (the molecule has an effect against the specific target or pathogen)\n",
        "*   Inactive: labelled with 0 (the molecule does not have an effect against the specific target or pathogen)\n",
        "\n",
        "\n",
        "### 2.1.1 Data Preparation\n",
        "\n",
        "We will model a critical step in the drug discovery development, **cardiotoxicty due to hERG blockade**. hERG is a potassium channel whose blockage causes prolongued QT intervals and eventually cardiotoxicity, and is one of the major adverse drug reactions that cause compound attrition in the drug discovery pipelines. Therefore, it is essential to identify putative hERG blockers early on the drug discovery cascade."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we first connect to drive\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IzZKQb0AchLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-2d8HaIbBM4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download TDC hERG Dataset\n",
        "#@markdown Click on the play button to get the train and tests sets for the TDC hERG data\n",
        "#@markdown By running this cell, you will get the datasets saved to the drive in data/session2\n",
        "\n",
        "%%capture\n",
        "\n",
        "#TDC can be installed and accessed as a Python package\n",
        "!pip install PyTDC\n",
        "\n",
        "# import the hERG dataset, part of the Toxicity data available at TDC\n",
        "from tdc.single_pred import Tox\n",
        "data = Tox(name = \"hERG\")\n",
        "\n",
        "split = data.get_split()\n",
        "\n",
        "#we can now separate the compressed dataset in the three sections\n",
        "train = split[\"train\"]\n",
        "validation = split[\"valid\"]\n",
        "test = split[\"test\"]\n",
        "\n",
        "train.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_herg_train.csv\", index=False)\n",
        "validation.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_herg_validation.csv\", index=False)\n",
        "test.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_herg_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7_4FYhnQ2um"
      },
      "source": [
        "#### Data Splits\n",
        "We will start by splitting the data in 3 parts:\n",
        "\n",
        "*   Train: the portion of the dataset that will be used to train the model\n",
        "*   Valid: the validation set, which will be used during the modelling to assert and improve the performance of the algorithm\n",
        "*   Test: a portion of the dataset kept completely separated from the data used to train the model. We will use this to evaluate the final model performance.\n",
        "\n",
        "Usually, this process is done manually and must ensure that the three datasets are balanced (i.e. have an equal representation of positives and negatives and are representative of all the chemical space). \n",
        "\n",
        "For the purpose of the course, we will use the prepared TDC split which already guarantees the above conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENcfLr9oQyxB"
      },
      "outputs": [],
      "source": [
        "#we can check how many molecules we have in each set\n",
        "print(len(train))\n",
        "print(len(validation))\n",
        "print(len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In this example, we will only use the train and test sets, since we will not be testing different model parameters*"
      ],
      "metadata": {
        "id": "hP00qXMaJrKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raroUvgcUC6E"
      },
      "outputs": [],
      "source": [
        "#let's explore the data\n",
        "#by convention, the input (molecules in this case) is named X, and the output (bioactivity) is Y\n",
        "train.head() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IsUvtsyUQVQ"
      },
      "outputs": [],
      "source": [
        "#we can check the number of positives and negatives in the train set\n",
        "print(len(train[train[\"Y\"]==0])) #not cardiotoxic\n",
        "print(len(train[train[\"Y\"]==1])) #cardiotoxic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ycwnIyUm1P"
      },
      "source": [
        "#### Data Visualization\n",
        "Finally, before moving onto model training, we can leverage the Python Package RdKit, the largest open source toolbox for chemioinformatics, to explore a bit more the chemical structures of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKyz7CAdVCn8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#we first need to install rdkit in Google Colab and import the packages of interest\n",
        "!pip install rdkit\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjQyRQBuVNX3"
      },
      "outputs": [],
      "source": [
        "#we select a list of smiles\n",
        "\n",
        "smiles = train[\"Drug\"][449:] #the last 9 molecules\n",
        "smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5lHvLc0VVz4"
      },
      "outputs": [],
      "source": [
        "#convert the smiles to RdKit Molecules (not readable by humans)\n",
        "mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n",
        "mols[0] #again, we can check what is the \"mols\" we have created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qn54PERVdB0"
      },
      "outputs": [],
      "source": [
        "#use the Draw function to visualise the molecules\n",
        "Draw.MolsToGridImage(mols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6BEWeoWgtQ"
      },
      "source": [
        "### 2.1.2 Molecule Featurization\n",
        "\n",
        "To train an ML model, we need to be able to pass the molecules to the computer in a format that the computer can understand. That is, numerical vectors or images.\n",
        "\n",
        "In this case, we will use the Chemical Checker to create signatures encompassing not only structural characteristics but also the bioactivity profile of the molecules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qosoZqS0W6Ah"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#first, we install the signaturizer and import it\n",
        "!pip install signaturizer\n",
        "from signaturizer import Signaturizer\n",
        "sign = Signaturizer(\"GLOBAL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1iIZFVIXImL"
      },
      "outputs": [],
      "source": [
        "#we then convert the smiles into vectors (X)\n",
        "X_train = sign.predict(train[\"Drug\"]).signature\n",
        "X_test = sign.predict(test[\"Drug\"]).signature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QmZFHvQXfM-"
      },
      "outputs": [],
      "source": [
        "# we can see how a molecule in the train set has been converted to a vector\n",
        "X_train[0] #0 indicates the first molecule in the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrodQG1RXdB3"
      },
      "outputs": [],
      "source": [
        "#finally, we need to prepare the outputs (Y), creating three lists:\n",
        "Y_train = list(train[\"Y\"])\n",
        "Y_test = list(test[\"Y\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk_M-lAAX6Ee"
      },
      "source": [
        "### 2.1.3. Supervised Machine Learning\n",
        "\n",
        "We will use the SciKit-Learn Python package to train an ML model based on a Random Forest algorithm. In this case, since the data is already binarized (0 and 1 for inactive and active, instead of continuous experimental results like IC50) we will train a Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8MQBSsIY--h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "#install scikit-learn (sklearn) and import the RandomForest function\n",
        "!pip install sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejLHhLWpZIa6"
      },
      "outputs": [],
      "source": [
        "#in ML, the training of a model is called \"fitting\" the model to the data (the molecules and outputs of the Train set)\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGCvU3MW13RJ"
      },
      "source": [
        "#### 2.1.4 Understanding Classification outputs\n",
        "\n",
        "A Classifier gives back two numbers:\n",
        "\n",
        "\n",
        "*   Probability of 0 (inactive)\n",
        "*   Probability of 1 (active)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avy40NHM2Fxz"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict_proba(X_test)\n",
        "y_pred[:10] #let's check the first 10 results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFFX9TV74MaD"
      },
      "outputs": [],
      "source": [
        "y_pred[:,1] #we are interested only in the probability of active (proba1 or second column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA_1xJ04Zc0c"
      },
      "source": [
        "#### 2.1.4 Model Evaluation\n",
        "To understand whether a model is performing correctly or not, we have several measures. Here, we will use two of them:\n",
        "\n",
        "\n",
        "*   Confusion matrices: a table that indicates how many molecules were correctly classified by the model and how many were misclassified.\n",
        "*   ROC Curve: a probability curve showing the True Positive and False Positive Rates at different classification thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzlMFHZkZUsD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay as cdm\n",
        "\n",
        "cdm.from_estimator(clf, X_test, Y_test) #we use the test set to check model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCJ-eA7ZFg3p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "#precision and recall scores need a specific threshold\n",
        "\n",
        "y_pred_bin = []\n",
        "for x in y_pred[:,1]:\n",
        "    if x > 0.5:\n",
        "        y_pred_bin += [1]\n",
        "    if x <= 0.5:\n",
        "        y_pred_bin += [0]\n",
        "precision = precision_score(Y_test, y_pred_bin)\n",
        "recall = recall_score(Y_test, y_pred_bin)\n",
        "print(precision, recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSL96Dq9aSyK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay as rdc\n",
        "\n",
        "rdc.from_estimator(clf, X_test, Y_test) #we use the test set to check model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73zp_0rfaXVZ"
      },
      "outputs": [],
      "source": [
        "#we can also try to predict the values in the training set and see how good our algorithm does:\n",
        "cdm.from_estimator(clf, X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQcfliszaiHj"
      },
      "outputs": [],
      "source": [
        "rdc.from_estimator(clf, X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "#precision and recall scores need a specific threshold\n",
        "\n",
        "train_pred = clf.predict_proba(X_train)[:,1]\n",
        "y_pred_bin = []\n",
        "for x in train_pred:\n",
        "    if x > 0.5:\n",
        "        y_pred_bin += [1]\n",
        "    if x <= 0.5:\n",
        "        y_pred_bin += [0]\n",
        "precision = precision_score(Y_train, y_pred_bin)\n",
        "recall = recall_score(Y_train, y_pred_bin)\n",
        "print(precision, recall)"
      ],
      "metadata": {
        "id": "uBwb8bUEL35h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtSEarv4h5n1"
      },
      "source": [
        "## 2.2 Regression Task\n",
        "Regression models predict continuous numerical values. Applied to the biomedical field, a regression model might predict the IC50 against a pathogen, or the % of inhibition of a specific target. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHfXVu44ieTF"
      },
      "source": [
        "### 2.2.1 Data Preparation\n",
        "We will use the Acute Toxicity LD50 dataset from TDCommons. It contains over 7000 molecules and its associated Lethal Dose 50 (the amount of an ingested substance in mg/kg that kills 50 percent of a test sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download TDC LD50 Dataset\n",
        "#@markdown Click on the play button to get the train and tests sets for the TDC hERG data\n",
        "#@markdown By running this cell, you will get the datasets saved to the drive in data/session2\n",
        "\n",
        "%%capture\n",
        "\n",
        "#TDC can be installed and accessed as a Python package\n",
        "!pip install PyTDC\n",
        "\n",
        "# import the LD50 dataset, part of the Toxicity data available at TDC\n",
        "from tdc.single_pred import Tox\n",
        "data = Tox(name = 'LD50_Zhu')\n",
        "\n",
        "split = data.get_split()\n",
        "\n",
        "#we can now separate the compressed dataset in the three sections\n",
        "train = split[\"train\"]\n",
        "validation = split[\"valid\"]\n",
        "test = split[\"test\"]\n",
        "\n",
        "train.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_ld50_train.csv\", index=False)\n",
        "validation.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_ld50_validation.csv\", index=False)\n",
        "test.to_csv(\"drive/MyDrive/h3d_ersilia_ai_workshop/data/session2/tdc_ld50_test.csv\", index=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Oz6FSwzHdBLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2stqnL4WkzSN"
      },
      "outputs": [],
      "source": [
        "#we can check how many molecules we have in each set\n",
        "print(len(train))\n",
        "print(len(validation))\n",
        "print(len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Again, we will only be using the train and test sets for this example*"
      ],
      "metadata": {
        "id": "Xs5PvEh3Nch0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWGyZjBVlDSl"
      },
      "outputs": [],
      "source": [
        "#explore the data\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1YmNekhlGbp"
      },
      "outputs": [],
      "source": [
        "# in this case, we can visualise the data not as active / inactive but as a distribution.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(train[\"Y\"], bins=50, color = \"#50285a\")\n",
        "plt.xlabel(\"Lethal Dose 50\")\n",
        "plt.ylabel(\"Number of molecules\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW3luRemmJ_d"
      },
      "source": [
        "### 2.2.2 Featurization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnq2D_RBFI2T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#first, we install the signaturizer and import it if we are running the notebook again, else it is not necessary\n",
        "!pip install signaturizer\n",
        "from signaturizer import Signaturizer\n",
        "sign = Signaturizer(\"GLOBAL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf-nEQ5imcYu"
      },
      "outputs": [],
      "source": [
        "#we then convert the smiles (X)\n",
        "X_train = sign.predict(train[\"Drug\"]).signature\n",
        "X_test = sign.predict(test[\"Drug\"]).signature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "zBAhQHk0N3A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xcsDVNOm_mc"
      },
      "outputs": [],
      "source": [
        "#finally, we need to prepare the outputs (Y), creating three lists:\n",
        "Y_train = list(train[\"Y\"])\n",
        "Y_test = list(test[\"Y\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[0]"
      ],
      "metadata": {
        "id": "8GdzsbykN1E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9u6oJcSnbX_"
      },
      "source": [
        "### 2.2.3 Model Training\n",
        "We will use the simplest regression algorithm, a Linear Regression, and evaluate how well it performs with our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwfp7DADnAx2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#import the Linear Regression function\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcQYw_Y6nQtl"
      },
      "outputs": [],
      "source": [
        "#in ML, the training of a model is called \"fitting\" the model to the data (the molecules and outputs of the Train set)\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC9TFAx-nlFR"
      },
      "outputs": [],
      "source": [
        "#we can now predict the outcome of the test set\n",
        "y_pred = reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj2YT9orokW8"
      },
      "outputs": [],
      "source": [
        "#if we check, the prediction is simply a list of LD50\n",
        "y_pred[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv8QbJEioxRt"
      },
      "source": [
        "### 2.2.4 Model Evaluation\n",
        "The best way to understand if the model predictions are accurate is to plot a scatter distribution of real vs predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61EIUs0Goojt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "ax.scatter(x=y_pred, y=Y_test, color=\"#dca0dc\")\n",
        "ax.set_xlabel(\"Predicted LD50\")\n",
        "ax.set_ylabel(\"Real LD50\")\n",
        "#for better visualisation, we also plot a diagonal line\n",
        "ax.plot([np.array(Y_test).min(), np.array(Y_test).max()], [np.array(Y_test).min(), np.array(Y_test).max()], 'r--', lw=3, color = \"#50285a\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAj1HTWbqw1e"
      },
      "source": [
        "In additon, we can use the following metrics:\n",
        "*   Mean Absolute Error (MAE)\n",
        "*   Mean Squared Error (MSE)\n",
        "*   R2 score\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k7hLTPbpR_N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "mae = mean_absolute_error(Y_test, y_pred)\n",
        "mse = mean_squared_error(Y_test, y_pred)\n",
        "r2 = r2_score(Y_test, y_pred)\n",
        "\n",
        "print(\"MAE: {}\".format(mae))\n",
        "print(\"MSE: {}\".format(mse))\n",
        "print(\"R2: {}\".format(r2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgaU6Gdprpgo"
      },
      "source": [
        "### 2.2.5 Improving model performance\n",
        "We have used the simplest model, a linear regression, which is unable to produce accurate predictions for the LD50.\n",
        "We can try with a more advanced ML algorithm, like a RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEL_gvwarWXj"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "reg = RandomForestRegressor(n_estimators=10) #we use only 10 trees for speed time\n",
        "reg.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGHQeGcKsCA-"
      },
      "outputs": [],
      "source": [
        "y_pred = reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x_iVjaotD8I"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "ax.scatter(x=y_pred, y=Y_test, color=\"#dca0dc\")\n",
        "ax.set_xlabel(\"Predicted LD50\")\n",
        "ax.set_ylabel(\"Real LD50\")\n",
        "ax.plot([np.array(Y_test).min(), np.array(Y_test).max()], [np.array(Y_test).min(), np.array(Y_test).max()], 'r--', lw=3, color = \"#50285a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikPBmXZotJAQ"
      },
      "outputs": [],
      "source": [
        "mae = mean_absolute_error(Y_test, y_pred)\n",
        "mse = mean_squared_error(Y_test, y_pred)\n",
        "r2 = r2_score(Y_test, y_pred)\n",
        "\n",
        "print(\"MAE: {}\".format(mae))\n",
        "print(\"MSE: {}\".format(mse))\n",
        "print(\"R2: {}\".format(r2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5-ZBeq3wJyJ"
      },
      "source": [
        "# Conclusions:\n",
        "\n",
        "We have learnt the basic steps to train a classification and a regression machine learning model based on bioactivity data. If this was a real-case scenario we would now use the validation results to improve the parameters on the algorithms and obtain more accurate predictions. Another avenue to improve predictions could also be testing different featurization steps for the molecules.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eGCvU3MW13RJ",
        "uA_1xJ04Zc0c"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}